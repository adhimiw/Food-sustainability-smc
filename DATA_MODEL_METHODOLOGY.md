# FoodFlow AI: Data, Model Choices, and Methodology Guide

This guide explains:
- what data exists in this project,
- why each model/algorithm was chosen,
- how the full methodology works end to end,
- and how you can improve it like a production system.

It is written as a learning document tied directly to this codebase.

## 1. Project Objective

FoodFlow AI is designed to reduce food waste in retail supply chains by combining:
- demand forecasting,
- surplus redistribution ("waste cascade"),
- route optimization,
- and carbon impact accounting.

Core idea:
1. predict demand,
2. detect likely surplus before expiry,
3. route surplus to better destinations (retailer -> food bank -> compost),
4. track financial and carbon impact.

## 2. Data Overview

Data is synthetic (generated), but structured to look realistic and correlated.
Main generator: `data/seed_database.py`.

### 2.1 Time and scope

- Date range: `2024-01-01` to `2025-12-31`
- Cities: `Metro City`, `Green Valley`, `Harbor Town`
- Store types: `retailer`, `food_bank`, `compost_facility`, `warehouse`
- Product categories include perishables and non-perishables.

### 2.2 Core raw tables (SQLite)

Defined in `database/db.py` and populated in `data/seed_database.py`.

1. `products`
- Product master data: category, shelf life, unit cost/price, carbon factor proxy.

2. `stores`
- Store and hub locations, type, capacity, city, coordinates.

3. `suppliers`
- Supplier master data and reliability/capacity features.

4. `supplier_products`
- Many-to-many link between suppliers and products.

5. `weather`
- Daily city-level weather (temperature, humidity, precipitation, condition).

6. `events`
- Calendar events with impact multipliers and affected categories.

7. `sales`
- Main fact table: ordered, sold, wasted quantities, revenue, waste cost, weather/event flags, calendar fields.

8. `inventory`
- Snapshot table for quantity-on-hand, freshness, and days until expiry.

### 2.3 Exported analytics datasets

Generated by `data/export_csv.py` and `data/export_random_csv_bundle.py`:
- daily/store/category summaries,
- risk matrices for perishables,
- leaderboard and performance tables,
- demand-feature datasets,
- carbon summaries.

These are useful for dashboards, Kaggle uploads, or external BI tools.

## 3. Why this data design?

The design balances realism and speed:
- **Relational core** (`sales`, `products`, `stores`) supports joins for ML features and dashboards.
- **External signals** (`weather`, `events`) introduce real-world demand variation.
- **Operational state** (`inventory`) enables proactive surplus handling.
- **Geospatial columns** (lat/lon) make route optimization possible.

This is enough to demonstrate full pipeline behavior without requiring proprietary data.

## 4. Model and Algorithm Choices

## 4.1 Demand Forecasting (`models/demand_forecaster.py`)

Current implementation uses **XGBoost Regressor**.

### Why XGBoost was chosen

- Handles nonlinear relationships well (weather/event/season interactions).
- Works with mixed engineered features without heavy preprocessing.
- Strong baseline for tabular demand forecasting.
- Fast enough for hackathon-scale retraining.

### Features used

- Temporal: day-of-week, month, week-of-year, quarter, weekend.
- Cyclical encodings: sin/cos for day-of-week/month/day-of-year.
- Lag features: 1, 3, 7, 14, 30 day lags.
- Rolling stats: mean/std/max windows.
- Trend proxy over recent window.
- Product/inventory context where available.
- Weather-derived signals (`is_hot`, `is_cold`, squared temperature).

### Training methodology

- Time-ordered split (80/20), not random split.
- Metrics: MAE and MAPE.
- Confidence interval approximation: prediction +/- 1.96 * MAE.

### Practical note

The file mentions Prophet in comments/import fallback, but active forecasting logic is XGBoost-driven in this code.

## 4.2 Waste Cascade Optimizer (`models/waste_cascade.py`)

This module is a **rule-based, tiered allocation heuristic**.

### Why this approach was chosen

- Encodes policy directly: feed people first, compost last.
- Easy to explain to non-technical stakeholders.
- Low computational cost and deterministic behavior.
- Good for operational prototypes where explainability is critical.

### Method

1. Identify surplus:
- `surplus_qty = quantity_on_hand - (daily_demand * forecast_days)`
- Force urgency for near-expiry inventory.

2. Priority scoring:
- freshness risk,
- surplus proportion,
- carbon intensity,
- expiry urgency.

3. Tier allocation:
- Tier 1: retailer -> nearby retailer (same city, distance threshold)
- Tier 2: remaining edible -> food bank
- Tier 3: remainder -> compost facility

Outputs are persisted to `waste_cascade_actions` and carbon logs.

## 4.3 Route Optimizer (`models/route_optimizer.py`)

Uses **OR-Tools VRP** when available; otherwise greedy nearest-neighbor fallback.

### Why OR-Tools

- Industry-standard library for routing and constraints.
- Supports capacity and route-distance constraints.
- Produces better routes than naive manual sequencing.

### Why fallback exists

- Makes app robust even when OR-Tools is unavailable.
- Ensures demo continuity in restricted environments.

### Current constraints

- vehicle capacity,
- max route distance/time envelope,
- multi-vehicle routing from a depot concept.

## 4.4 Carbon Calculator (`models/carbon_calculator.py`)

Deterministic accounting using category-level emission factors and transport emission rates.

### Why deterministic instead of ML

- Carbon conversion is rule-based domain math, not a pattern-learning task.
- Easier to audit and communicate.
- Reliable for KPI tracking and what-if estimations.

### Example logic

- Saved carbon from waste prevention:
`production_emission + landfill_avoidance`

- Redistribution net:
`saved_carbon - transport_emission`

## 5. End-to-End Methodology (Pipeline)

Implemented operationally in `run.py`:

1. Seed database (`data/seed_database.py`)
2. Train demand forecaster (`models/demand_forecaster.py`)
3. Identify and optimize surplus cascade (`models/waste_cascade.py`)
4. Optimize routes (`models/route_optimizer.py`)
5. Serve API (`api/main.py`) and dashboard (`dashboard/app.py`)

This follows a practical ML Ops loop:
- data -> model -> decision -> logistics -> impact -> monitoring.

## 6. Key Formulas Used

1. Waste rate:
`waste_rate = qty_wasted / (qty_sold + qty_wasted) * 100`

2. Surplus estimate:
`surplus_qty = quantity_on_hand - predicted_demand * horizon_days`

3. Transport CO2:
`co2 = emission_per_km_per_tonne * distance_km * (load_kg / 1000)`

4. Compost savings proxy:
`(landfill_factor - compost_factor) * quantity_kg`

## 7. Why this methodology works for hackathon and teaching

- Covers full stack: data engineering + ML + optimization + API + dashboard.
- Every decision is interpretable (important for sustainability use cases).
- Easy to demonstrate impact with clear KPIs (waste, cost, CO2).
- Modular architecture allows each component to evolve independently.

## 8. Current limitations (important for real-world use)

1. Synthetic data bias
- Real operations may have different error patterns and missingness.

2. Forecast uncertainty
- CI is heuristic (MAE-based), not full probabilistic modeling.

3. Cascade optimization
- Rule-based allocation, not full network flow optimization.

4. Routing fidelity
- No explicit time windows per stop, traffic, driver shifts, or pickup-delivery precedence in current simplified setup.

5. Carbon factors
- Category averages; should be supplier/product specific in production.

## 9. How to improve this system

1. Forecasting
- Add hierarchical forecasting (store-category -> store-SKU reconciliation).
- Compare XGBoost vs LightGBM/CatBoost vs temporal neural models.
- Add rolling backtests by month/store.

2. Optimization
- Replace heuristic cascade with min-cost flow / linear programming.
- Add hard constraints: food bank capacity, shelf life at arrival, SLA windows.

3. Routing
- Model pickup-delivery pairs, time windows, service times, and live traffic.
- Use route re-optimization on new incoming actions.

4. Data quality
- Build validation tests for negative/invalid quantities and outliers.
- Add reproducible data versioning for experiments.

5. Monitoring
- Track model drift, forecast error by category/store, and realized redistribution outcomes.

## 10. How to run and inspect quickly

```bash
# Full pipeline
python run.py

# Seed only
python data/seed_database.py

# API
uvicorn api.main:app --reload

# Dashboard
streamlit run dashboard/app.py

# Export CSV bundle
python data/export_random_csv_bundle.py --output-dir data/kaggle_bundle

# Load into DuckDB
python data/load_csv_to_duckdb.py --csv-root data/kaggle_bundle --duckdb-path data/foodflow.duckdb --overwrite
```

## 11. Learning checklist (study order)

1. Read schema in `database/db.py`
2. Read synthetic generation in `data/seed_database.py`
3. Read feature engineering in `models/demand_forecaster.py`
4. Read tier logic in `models/waste_cascade.py`
5. Read routing in `models/route_optimizer.py`
6. Read API contracts in `api/main.py`
7. Validate outputs in dashboard and exported CSVs

If you follow this order, you will understand both the "why" and the "how" of the project.
